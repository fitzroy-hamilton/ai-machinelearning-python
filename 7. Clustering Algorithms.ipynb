{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Tutorial #7 - Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/TannerGilbert/Tutorials/blob/master/Scikit-Learn-Tutorial/7.%20Clustering%20Algorithms.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/TannerGilbert/Tutorials/blob/master/Scikit-Learn-Tutorial/7.%20Clustering%20Algorithms.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scikit Learn Logo](http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  What is Clustering?\n",
    "\n",
    "Clustering is a technique that involves the grouping of data points in such a way that the data points in the same group(cluster) are more similar (in some sense) to each other than to those in other groups. Clustering is a unsupervised learning method which is commonly used for data analysis in Machine Learning and many other fields. In Machine Learning and Data Science clustering can give use important insights of our data by showing us the groups the data points fall into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because in Scikit Learn the syntax is almost the same for all algorithms it won't be the focus of this tutorial. We will rather focus on what algorithms are available and when to use which algorithm without focusing on the math behind each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means\n",
    "\n",
    "K Means groups the data into K clusters. The algorithm works iteratively to assign each data point to one of the K clusters bases on similarity of the features. You can see an example of K Means in the graphic below.  \n",
    "![](https://upload.wikimedia.org/wikipedia/commons/d/d5/Kmeans_animation.gif)  \n",
    "Steps:\n",
    "1. Select number of groups(clusters) and randomly initialize their respective center points(centroids).\n",
    "2. Assign data points to the cluster with the nearest centroid.\n",
    "3. Recompute centroid position by taking the mean of all data points assigned to the cluster.\n",
    "4. Repeat steps 2 and 3 until a set number of iterations or until the centroids aren't moving much between iterations anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Shift\n",
    "\n",
    "Mean shift clustering is a sliding-window-based algorithm that aims to discover “blobs” in a smooth density of samples. It is a centroid-based algorithm meaning that it wants to find the center points of each cluster, which works by updating candidates for centroids to be the mean of the points within the sliding-window. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids. You can see an example of Mean Shift in the graphic below.\n",
    "![](https://i.gifer.com/8OHN.gif)  \n",
    "Steps:\n",
    "1. Create sliding windows\n",
    "2. Each of the sliding windows are shifted towards regions of higher density by shifting there centroid(center of the sliding window) to the mean of the datapoints within the sliding window. This step will be repeated until there is no shift that yields a higher density(number of points in the sliding window)\n",
    "3. Selection of sliding windows by deleting overlapping windows. When multiple sliding windows overlap the window containing the most points is preserved and the other one is deleted.\n",
    "4. Assigning the data points to the sliding window in which they reside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical clustering\n",
    "\n",
    "Hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. There are 2 types of Hierarchical clustering: <b>Agglomerative</b> (bottom up) and <b>Divisive</b> (top down). Bottom up algorithms treat each data point as a single cluster and the merge the closest clusters and move up the hierarchy until all clusters have been merged and there is only a single cluster with all data points left. Top down algorithms are the opposite of the bottom up algorithms. They starts with one cluster that contains all data points and then performs splits and moves down the hierarchy. We can visualize the hierarchy using a <a href=\"https://en.wikipedia.org/wiki/Dendrogram\">dendrogram</a> or tree. In the graphic below you can see an example of how bottom up hierarchical clustering works.  \n",
    "![](https://camo.githubusercontent.com/ba07835dc2aba40a6644db39c9f9a595097ae25c/68747470733a2f2f6d656469612e67697068792e636f6d2f6d656469612f70534e435743454173677241732f67697068792e676966)   \n",
    "Steps (bottom up):\n",
    "1. Treat each data point as a single cluster.\n",
    "2. Choose a measure of similarity/dissimilarity  (distance metric like euclidean distance as a measure of similarity and an linkage criterion which specifies the dissimilarity of sets as a function of the pairwise distances of observations).\n",
    "3. Combine the two clusters with the smallest linkage i.e the two clusters that are closest together according to our chosen measure.\n",
    "4. Repeat 3 until we only have one cluster with all data points.\n",
    "5. Choose how many cluster we want by looking at the dendrogram.\n",
    "\n",
    "<img style=\"height:600px;\"  src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/12/Iris_dendrogram.png/800px-Iris_dendrogram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Density-Based Spatial Clustering of Applications with Noise  (DBSCAN)\n",
    "\n",
    "DBSCAN is a density based clustering algorithm just like Mean Shift. Given a set of data points it groups together points that are closely packed together(points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). DBSCAN requires two parameters: ε (eps) and the minimum number of points required to form a dense region (min_samples).  \n",
    "![](https://firebasestorage.googleapis.com/v0/b/programmingwithgilbert.appspot.com/o/Videos%2FScikit%20Learn%20Tutorials%2FScikit%20Learn%20Tutorial%20%237%20-%20Clustering%20Algorithms%2Fdbscan.gif?alt=media&token=2d494243-c3ab-4e7e-bf08-c3177357ba86)    \n",
    "Steps:\n",
    "1. DBSCAN begins with an arbitrary point.\n",
    "2. Extract neighbors (all points that are in ε distance to the first point)\n",
    "3. If there are a sufficient number of points (min_samples) within the neighborhood (ε distance) of the current point then the clustering process starts and the current data point gets added to the new cluster. Oterwise, the point will be labeled as a outlier or noise.\n",
    "4. The points within its ε distance neighborhood of the first point also become part of the same cluster. This procedure of making all points in the ε neighborhood belong to the same cluster is then repeated for all of the new points that have been just added to the cluster group.\n",
    "5. Repeat until all points in the cluster are determined (labeled).\n",
    "6. Once the cluster is done a new arbitrary point is chosen and is classified as either the first point of a new cluster or noise. This process repeats until all points are visited.   \n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/DBSCAN-density-data.svg/353px-DBSCAN-density-data.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Algorithms\n",
    "\n",
    "Because we aren't working with supervised data we can't get a easy measurement of how good our algorithm is so I decided to first show you how you can use clustering algorithms to fit and predict data and then show you an comparision made with the code of the offical <a href=\"http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\">Scikit Learn clustering tutorial</a>. I only made one change to the code. I removed all clustering algorithms that I didn't explain in this tutorial from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width\n",
       "0           5.1          3.5           1.4          0.2\n",
       "1           4.9          3.0           1.4          0.2\n",
       "2           4.7          3.2           1.3          0.2\n",
       "3           4.6          3.1           1.5          0.2\n",
       "4           5.0          3.6           1.4          0.2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "iris = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'label'])\n",
    "iris.drop(['label'], axis=1, inplace=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MeanShift, AgglomerativeClustering, DBSCAN\n",
    "\n",
    "models = [\n",
    "    ('KM', KMeans(n_clusters=3)),\n",
    "    ('MS', MeanShift()),\n",
    "    ('AC', AgglomerativeClustering()),\n",
    "    ('DBSCAN', DBSCAN())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 2 0 2 2 2 2\n",
      " 2 2 0 0 2 2 2 2 0 2 0 2 0 2 2 0 0 2 2 2 2 2 0 2 2 2 2 0 2 2 2 0 2 2 2 0 2\n",
      " 2 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0]\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 -1  0  0  0  0  0  0\n",
      "  0  0  1  1  1  1  1  1  1 -1  1  1 -1  1  1  1  1  1  1  1 -1  1  1  1\n",
      "  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1  1\n",
      "  1  1 -1  1  1  1  1  1  1 -1 -1  1 -1 -1  1  1  1  1  1  1  1 -1 -1  1\n",
      "  1  1 -1  1  1  1  1  1  1  1  1 -1  1  1 -1 -1  1  1  1  1  1  1  1  1\n",
      "  1  1  1  1  1  1]\n"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    clf = model\n",
    "    print(clf.fit_predict(iris))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://firebasestorage.googleapis.com/v0/b/programmingwithgilbert.appspot.com/o/Videos%2FScikit%20Learn%20Tutorials%2FScikit%20Learn%20Tutorial%20%237%20-%20Clustering%20Algorithms%2FAlgorithm%20Comparison.png?alt=media&token=c71334ff-e81f-465d-8009-655fe0132017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"http://scikit-learn.org/stable/modules/clustering.html\">Clustering Overview (Scikit Learn)</a></li>\n",
    "    <li><a href=\"https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68\">The 5 Clustering Algorithms Data Scientists Need to Know (George Seif)</a></li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/K-means_clustering\">K Means (Wikipedia)</a></li>\n",
    "    <li><a href=\"http://www.chioka.in/meanshift-algorithm-for-the-rest-of-us-python/\">Mean Shift (Chioka.in)</a></li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Mean_shift\">Mean Shift (Wikipedia)</a></li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Hierarchical_clustering\">Hierarchical Clustering (Wikipedia)</a></li>\n",
    "    <li><a href=\"https://github.com/albert-cyberhulk/hierarchical-clustering-using-r-and-python\">Hierarchical Clustering Example (Albert Cyberhulk)</a></li>\n",
    "    <li><a href=\"https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/\">DBSCAN Interactive Graph</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That was a quick overview of some clustering algorithms and how to use them in Scikit Learn. \n",
    "I hope you liked this tutorial if you did consider subscribing on my <a href=\"https://www.youtube.com/channel/UCBOKpYBjPe2kD8FSvGRhJwA\">Youtube Channel</a> or following me on Social Media. If you have any question feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
