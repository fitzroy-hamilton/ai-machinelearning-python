{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les dépendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/Data.csv')\n",
    "#print(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iloc prend deux parametres : nombre de lignes et nombre de colonnes \":\" signifie que l'on prend toutes les lignes ou colonnes du dataset en faisant \":-1\" on part du principe que la derniere colonne contient la colonne de variables dependantes que l'on mettra dans le vecteur y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values # on recupere juste la derniere colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gérer les données manquantes\n",
    "Pour cela, on utilise la class Imputer du module preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Imputer' from 'sklearn.preprocessing' (/Users/kama/opt/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-92365dbc2961>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimputer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'NaN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Imputer' from 'sklearn.preprocessing' (/Users/kama/opt/anaconda3/lib/python3.8/site-packages/sklearn/preprocessing/__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lie l'imputer a notre variable en precisant les lignes puis colonnes que l'on souhaite. Pour rappel, dans un range, la borne superieure est toujours exclue en python donc 1:3 correspond aux colonnes dont l'indice est 1 et 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(X[:, 1:3])\n",
    "# on transforme la variable conformement a la strategie definie \n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "print(\"Independent dataset X:\")\n",
    "print(X)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gérer les variables catégoriques (ie non numériques continues)\n",
    "\n",
    "Ici dans notre dataset, les pays (3 categories) et purchase (2 categories). Le but est d'encoder les variables catégoriques (en texte par exemple) en variable numerique. \n",
    "\n",
    "Variable nominale (= non ordonnee) vs. ordinale (= ordonnee).\n",
    "\n",
    "La class LabelEncoder permet de transformer du texte en valeur numérique (1, 2, 3, ...) et la class OneHotEncoder permet de spliter en plusieurs colonnes une colonne contenant des valeurs numeriques (= pas de texte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder() # pour la variable country\n",
    "# fit_transform fait d'abord fit puis transform (en evitant des calculs en double)\n",
    "X[:, 0] = labelencoder_X.fit_transform(X[:, 0]) # country est la 1ere colonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Méthode d'encodage pour une variable catégorique nominale = encode par dummy-variable (obligatoire pour les variables independantes = entrees).\n",
    "\n",
    "L'encodage par dummy-variable consiste a decouper en N colonnes une variable categorique à N categories (ie. valeurs distinctes) puis de mettre 1 ou 0 dans chacune de ces colonnes (par exemple la valeur France de la colonne country est remplacée par un 1 dans la colonne France et 0 dans les colonnes Germany et Spain). Si on avait simplement remplacé les pays par un chiffre (0 pour la France, 1 pour Germany et 2 pour Spain), on aurait recréé une relation d'ordre entre les pays puisque 1 < 2 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder = OneHotEncoder(categorical_features = [0], dtype=int) # categorical_features correspond aux indices des colonnes a transformer\n",
    "np.set_printoptions(suppress=True)\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "print(\"Independent dataset X apres gestion des variables categoriques:\")\n",
    "print(X)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation de la variable 'purchase' textuelle en valeurs numériques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelencoder_y = LabelEncoder() \n",
    "y = labelencoder_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diviser le dataset entre le Training set et le Test set\n",
    "\n",
    "Le training set est la base d'apprentissage pour découvrir les corrélations entre les variables independantes (entrees) et les variables dependantes (sorties) afin d'eviter le sur-apprentissage (apprentissage par coeur), on  conserve une partie des donnees pour tester le modele d'apprentissage avec les valeurs attendues.\n",
    "\n",
    "Les observations du test set n'auront pas contribué à l'apprentissage et on dimensionne a 20% des observations totales, le nombre d'observations à mettre de coté pour le test set.\n",
    "\n",
    "Si la precision (taux d'erreur) du test set est significativement plus élevée que le train set, cela signifie qu'il y a eu sur-apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# on renseigne le random_state pour est sur d'avoir les mêmes resultats a \n",
    "# chaque execution : utile uniquement dans un but pedagogique ou de debugging\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "On met à la même echelle toutes les variables pour éviter qu'une variable d'entrée n'écrase toutes les autres (par exemple, dans notre dataset la colonne Age n'a pas du tout la même échelle que la colonne Salaire).\n",
    "\n",
    "On utilise ici la méthode de standardisation (qui consiste à soustraire aux valeurs de x la moyenne des valeurs de x le tout divisé par l'écart-type de x).\n",
    "\n",
    "Une autre methode est la methode de normalisation (qui consiste à soustraire aux valeurs de x la valeur minimum de x le tout divisé par la soustraction de la valeur maximum de x par la valeur minimum de x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "#print(X_train)\n",
    "X_train = sc.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce la fonction train_test_split, le training set et le test set ont une moyenne et un écart-type similaire et une distribution similaire des valeurs.\n",
    "\n",
    "On n'a donc pas besoin de fitter sc à X_test puisqu'il est déjà fitté à X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sc.transform(X_test)\n",
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On n'applique par le feature scaling sur y (sorties) puisque dans ce dataset, les valeurs ont déjà été encodées (avec LabelEncoder) en 0 ou 1 ce qui est dans une echelle similaire aux variables de X (entrees).\n",
    "\n",
    "En fonction du dataset, il peut être nécessaire d'appliquer le feature scaling aux sorties (typiquement si y est un vecteur de valeurs numériques avec une échelle très différentes des valeurs de X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
