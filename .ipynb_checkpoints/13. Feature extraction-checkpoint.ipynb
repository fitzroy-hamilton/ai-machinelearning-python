{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit Learn Tutorial #13 - Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\"><td>\n",
    "  <a target=\"_blank\"  href=\"https://colab.research.google.com/github/TannerGilbert/Tutorials/blob/master/Scikit-Learn-Tutorial/13.%20Feature%20extraction.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab\n",
    "  </a>\n",
    "</td><td>\n",
    "  <a target=\"_blank\"  href=\"https://github.com/TannerGilbert/Tutorials/blob/master/Scikit-Learn-Tutorial/13.%20Feature%20extraction.ipynb\">\n",
    "    <img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "</td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Scikit Learn Logo](http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is feature extraction important?\n",
    "\n",
    "Sometimes our data isn't in the right format for Machine Learning. Feature extraction can be used to extract features in a format supported by machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction in Scikit Learn\n",
    "\n",
    "Scikit Learns <i>sklearn.feature_extraction</i> provides a lot of different functions to extract features from something like text or images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading features from dicts (DictVectorizer)\n",
    "\n",
    "DictVectorizer can be used to transform your data from a Python dict to a Numpy array which can be used for Machine Learning. It also transforms categorical data into a one hot encoded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0., 33.],\n",
       "       [ 0.,  1.,  0., 12.],\n",
       "       [ 0.,  0.,  1., 18.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Creating array of dicts \n",
    "measurements = [\n",
    "    {'city': 'Dubai', 'temperature': 33.},\n",
    "    {'city': 'Londo', 'temperature': 12.},\n",
    "    {'city': 'San Francisco', 'temperature': 18.},\n",
    "]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "vec.fit_transform(measurements).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city=Dubai', 'city=Londo', 'city=San Francisco', 'temperature']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing the names of the new features\n",
    "vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature hashing (FeatureHasher)\n",
    "\n",
    "It is a high speed, low memory vectorizer which uses a technique known as feature hashing to vectorize data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1., -4.,  2.],\n",
       "       [-5., -2.,  0., -7.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Creating array of dicts\n",
    "data = [\n",
    "    {'dog': -1, 'cat': 2, 'elephant': 4},\n",
    "    {'dog': 2, 'run': 5, 'cat':-7}\n",
    "]\n",
    "\n",
    "h = FeatureHasher(n_features=4)\n",
    "h.transform(data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text feature extraction\n",
    "\n",
    "Scikit Learn offers multiple ways to extract numeric feature from text:\n",
    "<ul>\n",
    "    <li><b>tokenizing</b> strings and giving an integer id for each possible token.</li>\n",
    "    <li><b>counting</b> the occurrences of tokens in each document.</li>\n",
    "    <li><b>normalizing</b> and weighting with diminishing importance tokens that occur in the majority of samples / documents.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Creating Dataset\n",
    "data = [\n",
    "    'Test sentence one of three.',\n",
    "    'Second test sentence of three.',\n",
    "    'Last sentence of three.'\n",
    "]\n",
    "\n",
    "vec = CountVectorizer()\n",
    "\n",
    "vec.fit_transform(data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.transform(['New sentence']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In large texts there will be a lot of words like \"a\" and \"the\" which don't provide meaning to our classifier but rather trick our model. To prevent this we could run CountVectorizer and then delete all tokens that appear more the k percent or we could use Scikit Learns <i>TfidfTransformer</i> in combination with the CountVectorizer or <i>TfidfVectorizer</i> which combines both of them. These to functions put weights on the tokens. The weights are lower  the more often the token occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "data_vec = vec.transform(data).toarray()\n",
    "data_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.3645444 , 0.61722732, 0.        , 0.3645444 ,\n",
       "        0.46941728, 0.3645444 ],\n",
       "       [0.        , 0.3645444 , 0.        , 0.61722732, 0.3645444 ,\n",
       "        0.46941728, 0.3645444 ],\n",
       "       [0.69903033, 0.41285857, 0.        , 0.        , 0.41285857,\n",
       "        0.        , 0.41285857]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "data_vec_weighted = tfidf.fit_transform(data_vec)\n",
    "data_vec_weighted.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.3645444 , 0.61722732, 0.        , 0.3645444 ,\n",
       "        0.46941728, 0.3645444 ],\n",
       "       [0.        , 0.3645444 , 0.        , 0.61722732, 0.3645444 ,\n",
       "        0.46941728, 0.3645444 ],\n",
       "       [0.69903033, 0.41285857, 0.        , 0.        , 0.41285857,\n",
       "        0.        , 0.41285857]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "tfidf_vec.fit_transform(data).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"http://scikit-learn.org/stable/modules/feature_extraction.html\">Feature Extraction (Scikit Learn Documentation)</a></li>\n",
    "    <li><a href=\"https://datascience.stackexchange.com/questions/29006/feature-selection-vs-feature-extraction-which-to-use-when\">Feature selection vs Feature extraction. Which to use when? (stackexchange)</a></li>\n",
    "    <li><a href=\"https://en.wikipedia.org/wiki/Feature_extraction\">Feature Extraction (Wikipedia)</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That was a quick overview of feature extraction and how to implement it in Scikit Learn. \n",
    "I hope you liked this tutorial if you did consider subscribing on my <a href=\"https://www.youtube.com/channel/UCBOKpYBjPe2kD8FSvGRhJwA\">Youtube Channel</a> or following me on Social Media. If you have any question feel free to contact me."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
